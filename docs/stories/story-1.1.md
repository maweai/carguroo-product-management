# Story 1.1: Upload de Manuais Técnicos (PDF)

**Status**: ContextReadyDraft
**Epic**: Epic 1 - Infraestrutura RAG e Base de Conhecimento
**Priority**: CRÍTICA
**Story ID**: US-RAG-01

---

## Story

**Como** administrador da plataforma
**Quero** fazer upload de manuais técnicos em formato PDF via interface web
**Para que** eu possa alimentar a base de conhecimento do sistema RAG que será utilizado pelo Copiloto e Sistema de Avaliação

---

## Requirements Context Summary

### Source Documents
- **PRD**: `PRD-CarGuroo-2025-10-10.md` (FR-05: Ingestão e Processamento de Base de Conhecimento)
- **Epics**: `epics-CarGuroo-2025-10-10.md` (Epic 1, US-RAG-01)
- **Tech Specs**: `tech-specs-CarGuroo-2025-10-10.md` (Section 1.2: API Specifications)
- **Architecture**: `solution-architecture.md` (ADR-006: Storage on S3, ADR-001: Modular Monolith)

### Epic Context
**Epic 1**: Infraestrutura RAG e Base de Conhecimento (Foundation)
- **Objetivo**: Estabelecer fundação técnica compartilhada entre Copiloto e Avaliação, com capacidade de ingerir, processar e indexar documentos técnicos
- **Valor de Negócio**: Sem esta infraestrutura, nenhuma das funcionalidades core (Copiloto ou Avaliação) pode funcionar. É o "motor" de IA do sistema
- **Total Stories no Epic**: 9 (US-RAG-01 a US-RAG-09)
- **Esta story é a primeira** - não há pré-requisitos

### Functional Requirements (from PRD)
**FR-05**: O sistema deve ingerir, processar e indexar manuais técnicos (PDF), histórico de help desk e conteúdos de treinamento, realizando chunking, embedding e armazenamento em vector database.

**Escopo desta Story**: Upload de PDF + validação + armazenamento seguro + trigger de processamento assíncrono

### Architectural Constraints

#### Storage (from ADR-006)
- **Primary Storage**: AWS S3 (Standard tier)
- **Encryption**: Server-side encryption (SSE-S3 ou SSE-KMS) - NFR-05 (LGPD Compliance)
- **Bucket Structure**: `carguroo-documents-{env}` (separate buckets for dev/staging/prod)
- **Naming Convention**: `{document_id}/{original_filename}` (UUID-based para evitar colisões)

#### Component Boundaries (from solution-architecture.md, lines 462-484)
- **Module**: RAG Module (`/backend/rag`)
- **Sub-module**: Ingestion (`rag/ingestion`)
- **Responsibilities**:
  - Upload de documentos
  - Validação de formato e tamanho
  - Armazenamento em S3
  - Criação de registro de metadata em PostgreSQL (`rag.documents` table)
  - Trigger de processamento assíncrono (Celery task)

#### Security Requirements (from NFR-05, solution-architecture.md lines 946-998)
- **Authentication**: Apenas usuários com role `admin` podem fazer upload (RBAC enforcement)
- **In-Transit**: TLS 1.3 em todas as conexões HTTP
- **At-Rest**: S3 server-side encryption (SSE-S3 mínimo, SSE-KMS se exigido por compliance)
- **Audit Logging**: Registrar todas as operações de upload em `auth.audit_log` table
  - `action = 'document_uploaded'`
  - `resource_type = 'document'`
  - `resource_id = document.id`
  - `metadata = {filename, size_bytes, s3_key}`

#### Database Schema (from solution-architecture.md, lines 677-695)
**Table**: `rag.documents`
```sql
CREATE TABLE rag.documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(255) NOT NULL,
  s3_key VARCHAR(500) NOT NULL,
  file_size_bytes BIGINT,
  status VARCHAR(50) DEFAULT 'pending' CHECK (status IN ('pending', 'processing', 'completed', 'error')),
  uploaded_by UUID REFERENCES auth.users(id),
  uploaded_at TIMESTAMP DEFAULT NOW(),
  processed_at TIMESTAMP,
  total_chunks INTEGER,
  error_message TEXT
);

CREATE INDEX idx_documents_status ON rag.documents(status);
```

**Fields Populated by This Story**:
- `id`: Auto-generated UUID
- `name`: Original filename do PDF
- `s3_key`: Caminho completo no S3 (`{document_id}/{original_filename}`)
- `file_size_bytes`: Tamanho do arquivo em bytes
- `status`: Inicializado como `'pending'`
- `uploaded_by`: ID do admin que fez upload
- `uploaded_at`: Timestamp atual

#### Async Processing Trigger (from tech-specs.md, lines 269-344)
Após upload bem-sucedido, o sistema deve:
1. Criar registro na tabela `rag.documents` com `status='pending'`
2. Trigger background task: `process_document_task.delay(document.id)`
3. **Não bloquear** a resposta do endpoint esperando processamento (202 Accepted, não 200 OK)

**Celery Task** (será implementado em US-RAG-02/03):
```python
@shared_task(bind=True, max_retries=3)
def process_document_task(self, document_id: str):
    # Extract text → Chunk → Generate embeddings → Index in Qdrant
    # (implementação em próximas stories)
```

### Technical Implementation Notes (from tech-specs.md, lines 109-157)

#### Backend Endpoint
**Route**: `POST /api/v1/documents/upload`
**File**: `/backend/rag/routes.py`

**Request**:
```http
POST /api/v1/documents/upload
Content-Type: multipart/form-data
Authorization: Bearer <jwt_token>

{
  "file": <binary_pdf>,
  "metadata": {
    "document_type": "manual_tecnico",
    "vehicle_model": "XYZ-2023",
    "tags": ["freios", "motor", "eletrica"]
  }
}
```

**Response** (202 Accepted):
```json
{
  "document_id": "uuid",
  "name": "Manual_Tecnico_XYZ_2023.pdf",
  "size_bytes": 52428800,
  "status": "pending",
  "uploaded_at": "2025-10-10T14:30:00Z",
  "processing_eta_seconds": 180
}
```

**Validation Rules**:
1. **File Type**: Apenas `.pdf` extension allowed (case-insensitive)
   - Error 400: `"Only PDF files allowed"`
2. **File Size**: Máximo 50MB (52,428,800 bytes)
   - Error 413: `"File too large (max 50MB)"`
3. **User Role**: Apenas `role='admin'` pode fazer upload
   - Error 403: `"Insufficient permissions"`

**Error Handling**:
- **S3 Upload Failure**: Rollback transaction, não criar registro em DB
- **Database Insertion Failure**: Delete arquivo do S3, retornar erro 500
- **Celery Task Trigger Failure**: Log error, mas retornar 202 (task será retentado via monitoring)

#### S3 Service Integration
**Library**: `boto3` (AWS SDK for Python)

**Upload Flow**:
```python
# 1. Read file bytes
file_bytes = await file.read()

# 2. Generate S3 key
document_id = uuid.uuid4()
s3_key = f"{document_id}/{file.filename}"

# 3. Upload to S3
s3_client.put_object(
    Bucket=settings.S3_BUCKET_NAME,
    Key=s3_key,
    Body=file_bytes,
    ContentType='application/pdf',
    ServerSideEncryption='AES256'  # SSE-S3
)

# 4. Create DB record
document = Document(
    id=document_id,
    name=file.filename,
    s3_key=s3_key,
    file_size_bytes=len(file_bytes),
    uploaded_by=current_user.id
)
db.add(document)
db.commit()

# 5. Trigger async processing
process_document_task.delay(str(document_id))
```

#### Frontend Component
**Framework**: React + TypeScript
**Library**: `react-dropzone` (drag-and-drop interface)

**Component**: `DocumentUpload.tsx`
**Location**: `/frontend/src/components/documents/DocumentUpload.tsx`

**Features** (conforme AC):
1. Drag-and-drop zone (visual feedback on hover)
2. File picker button (fallback)
3. Validação client-side (file type + size) antes de enviar
4. Progress bar durante upload (usando `axios` progress events)
5. Lista de documentos "Pendentes de Processamento" (polling `/api/v1/documents?status=pending`)

**States**:
- `idle`: Aguardando upload
- `uploading`: Upload em progresso (mostrar progress bar)
- `success`: Upload concluído (mostrar documento na lista)
- `error`: Erro no upload (mostrar mensagem de erro)

### Testing Requirements

#### Unit Tests (pytest)
**File**: `/backend/tests/unit/test_rag_upload.py`

Testes a implementar:
1. `test_validate_pdf_extension` - Rejeitar arquivos não-PDF
2. `test_validate_file_size_under_limit` - Aceitar arquivos <50MB
3. `test_validate_file_size_over_limit` - Rejeitar arquivos >50MB
4. `test_admin_permission_required` - Apenas admin pode fazer upload
5. `test_s3_key_generation` - Verificar formato do S3 key (`{uuid}/{filename}`)

#### Integration Tests (pytest + docker-compose)
**File**: `/backend/tests/integration/test_rag_upload_flow.py`

Testes a implementar:
1. `test_full_upload_flow` - Upload end-to-end (file → S3 → DB → Celery trigger)
2. `test_s3_upload_with_encryption` - Verificar que arquivo foi encriptado no S3
3. `test_audit_log_created` - Verificar que log de auditoria foi criado
4. `test_duplicate_filename_handling` - Arquivos com mesmo nome não colidem (UUID diferente)

#### E2E Tests (Playwright)
**File**: `/frontend/tests/e2e/document-upload.spec.ts`

Cenários a implementar:
1. Upload via drag-and-drop (arquivo válido 10MB)
2. Upload via file picker (arquivo válido 40MB)
3. Validação client-side (arquivo 60MB rejeitado antes de envio)
4. Progress bar atualiza corretamente durante upload
5. Documento aparece em lista "Pendentes de Processamento" após upload

---

## Acceptance Criteria

### AC1: Interface web permite upload de arquivos PDF (tamanho máximo 50MB)
**Given** um administrador autenticado acessa a interface de gestão de documentos
**When** ele seleciona um arquivo PDF válido com tamanho ≤ 50MB para upload
**Then** o sistema aceita o arquivo e inicia o upload

**Test**: Upload de arquivo PDF de 10MB via drag-and-drop
**Expected**: Arquivo aceito, upload iniciado

---

### AC2: Sistema valida formato do arquivo (apenas PDF aceito)
**Given** um administrador tenta fazer upload de um arquivo
**When** o arquivo não tem extensão `.pdf`
**Then** o sistema rejeita o upload com mensagem "Only PDF files allowed"

**Test Cases**:
- Upload de `.docx` → Rejeitado
- Upload de `.jpg` → Rejeitado
- Upload de `.txt` → Rejeitado
- Upload de `.PDF` (uppercase) → Aceito (case-insensitive)

**Test**: Tentar upload de arquivo `manual.docx`
**Expected**: Erro 400 com mensagem "Only PDF files allowed"

---

### AC3: Upload mostra barra de progresso com % concluído
**Given** um arquivo PDF válido está sendo enviado
**When** o upload está em progresso
**Then** a interface mostra barra de progresso com percentual atualizado em tempo real

**Technical Implementation**:
```typescript
// Frontend (axios progress event)
const uploadFile = async (file: File) => {
  const formData = new FormData();
  formData.append('file', file);

  const response = await axios.post('/api/v1/documents/upload', formData, {
    onUploadProgress: (progressEvent) => {
      const progress = Math.round((progressEvent.loaded * 100) / progressEvent.total);
      setUploadProgress(progress); // Update progress bar
    }
  });
};
```

**Test**: Upload de arquivo 30MB, verificar que progress bar vai de 0% → 100%
**Expected**: Progress bar atualiza corretamente

---

### AC4: Após upload bem-sucedido, documento aparece em lista de "Pendentes de Processamento"
**Given** um arquivo PDF foi enviado com sucesso
**When** o backend retorna resposta 202 Accepted
**Then** o documento aparece na seção "Pendentes de Processamento" com status `pending`

**Technical Implementation**:
- Frontend faz polling `GET /api/v1/documents?status=pending` a cada 5 segundos
- Backend retorna lista de documentos com `status='pending'`
- UI mostra lista ordenada por `uploaded_at DESC`

**Test**: Após upload, verificar que documento aparece na lista com status "Pendente"
**Expected**: Documento visível na lista "Pendentes de Processamento"

---

### AC5: Sistema armazena PDF em storage seguro (S3 ou equivalente)
**Given** um arquivo PDF foi aceito para upload
**When** o upload é concluído
**Then** o arquivo está armazenado em S3 com server-side encryption ativado

**Verification**:
```python
# Backend test
s3_object = s3_client.get_object(Bucket=bucket, Key=s3_key)
assert s3_object['ServerSideEncryption'] == 'AES256'
```

**Test**: Verificar que arquivo no S3 tem `ServerSideEncryption='AES256'`
**Expected**: Encryption ativado

---

### AC6: Metadados básicos são coletados: nome do arquivo, tamanho, data de upload, uploader
**Given** um arquivo PDF foi enviado
**When** o sistema cria registro na tabela `rag.documents`
**Then** os seguintes metadados estão preenchidos:
- `name`: Filename original (e.g., "Manual_Tecnico_XYZ_2023.pdf")
- `file_size_bytes`: Tamanho exato em bytes
- `uploaded_at`: Timestamp do upload (UTC)
- `uploaded_by`: UUID do admin que fez upload

**Test**: Após upload, query DB e verificar que campos estão corretos
**Expected**: Todos os metadados preenchidos corretamente

---

## Tasks / Subtasks

### Backend Tasks

- [ ] **Task 1**: Criar rota de upload de documentos (AC: 1, 2) - `backend/rag/routes.py:upload_document`
  - [ ] Subtask 1.1: Implementar validação de file type (apenas .pdf)
  - [ ] Subtask 1.2: Implementar validação de file size (max 50MB)
  - [ ] Subtask 1.3: Implementar verificação de permissão (admin only)
  - [ ] Subtask 1.4: Retornar erros apropriados (400, 413, 403)

- [ ] **Task 2**: Implementar serviço de upload para S3 (AC: 5) - `backend/rag/services.py:DocumentService.upload_to_s3`
  - [ ] Subtask 2.1: Gerar S3 key único (`{uuid}/{filename}`)
  - [ ] Subtask 2.2: Upload com boto3 e SSE-S3 encryption
  - [ ] Subtask 2.3: Tratamento de erros de upload (rollback)
  - [ ] Subtask 2.4: Logging de upload bem-sucedido

- [ ] **Task 3**: Criar registro de documento em DB (AC: 6) - `backend/rag/services.py:DocumentService.create_document`
  - [ ] Subtask 3.1: Inserir em `rag.documents` com campos: id, name, s3_key, file_size_bytes, uploaded_by, uploaded_at
  - [ ] Subtask 3.2: Status inicial = `'pending'`
  - [ ] Subtask 3.3: Transaction handling (rollback se S3 upload falhar)

- [ ] **Task 4**: Trigger de processamento assíncrono (AC: 4) - `backend/rag/routes.py:upload_document`
  - [ ] Subtask 4.1: Chamar `process_document_task.delay(document.id)` após DB commit
  - [ ] Subtask 4.2: Não bloquear resposta (retornar 202 imediatamente)
  - [ ] Subtask 4.3: Calcular `processing_eta_seconds` estimado (baseado em file size)

- [ ] **Task 5**: Criar endpoint de listagem de documentos (AC: 4) - `backend/rag/routes.py:list_documents`
  - [ ] Subtask 5.1: Implementar `GET /api/v1/documents?status=pending`
  - [ ] Subtask 5.2: Filtrar por status, ordenar por `uploaded_at DESC`
  - [ ] Subtask 5.3: Paginação (limit=20 default)

- [ ] **Task 6**: Implementar audit logging (AC: 6) - `backend/auth/audit.py:log_document_upload`
  - [ ] Subtask 6.1: Inserir em `auth.audit_log` com action='document_uploaded'
  - [ ] Subtask 6.2: Incluir metadata: filename, size, s3_key

- [ ] **Task 7**: Escrever unit tests (Coverage: >80%)
  - [ ] Subtask 7.1: `test_validate_pdf_extension`
  - [ ] Subtask 7.2: `test_validate_file_size_under_limit`
  - [ ] Subtask 7.3: `test_validate_file_size_over_limit`
  - [ ] Subtask 7.4: `test_admin_permission_required`
  - [ ] Subtask 7.5: `test_s3_key_generation`

- [ ] **Task 8**: Escrever integration tests
  - [ ] Subtask 8.1: `test_full_upload_flow`
  - [ ] Subtask 8.2: `test_s3_upload_with_encryption`
  - [ ] Subtask 8.3: `test_audit_log_created`
  - [ ] Subtask 8.4: `test_duplicate_filename_handling`

### Frontend Tasks

- [ ] **Task 9**: Criar componente de upload com drag-and-drop (AC: 1, 3) - `frontend/src/components/documents/DocumentUpload.tsx`
  - [ ] Subtask 9.1: Integrar `react-dropzone` para drag-and-drop UI
  - [ ] Subtask 9.2: Implementar validação client-side (file type + size)
  - [ ] Subtask 9.3: Implementar progress bar (axios onUploadProgress)
  - [ ] Subtask 9.4: Mostrar estados: idle, uploading, success, error

- [ ] **Task 10**: Criar lista de documentos pendentes (AC: 4) - `frontend/src/components/documents/PendingDocumentsList.tsx`
  - [ ] Subtask 10.1: Implementar polling `GET /api/v1/documents?status=pending` (interval 5s)
  - [ ] Subtask 10.2: Mostrar lista com nome, tamanho, data de upload
  - [ ] Subtask 10.3: Auto-refresh ao detectar novo documento

- [ ] **Task 11**: Escrever E2E tests (Playwright)
  - [ ] Subtask 11.1: Test upload via drag-and-drop
  - [ ] Subtask 11.2: Test upload via file picker
  - [ ] Subtask 11.3: Test validação client-side (arquivo >50MB)
  - [ ] Subtask 11.4: Test progress bar update
  - [ ] Subtask 11.5: Test documento aparece em lista após upload

### Infrastructure Tasks

- [ ] **Task 12**: Configurar S3 bucket - `infra/terraform/s3.tf` (ou manual setup)
  - [ ] Subtask 12.1: Criar bucket `carguroo-documents-dev`
  - [ ] Subtask 12.2: Habilitar server-side encryption (SSE-S3)
  - [ ] Subtask 12.3: Configurar CORS para frontend uploads
  - [ ] Subtask 12.4: Configurar IAM policy (backend pode read/write, frontend não tem acesso direto)

- [ ] **Task 13**: Criar database migration - `backend/alembic/versions/001_create_documents_table.py`
  - [ ] Subtask 13.1: Migração para criar tabela `rag.documents`
  - [ ] Subtask 13.2: Migração para criar index `idx_documents_status`
  - [ ] Subtask 13.3: Executar migration em dev environment

---

## Dev Notes

### Alignment with Unified Project Structure

**Backend Structure**:
```
/backend
  /rag                      # RAG Module (Epic 1)
    /routes.py              # ← Implementar upload_document endpoint aqui
    /services.py            # ← DocumentService com upload_to_s3 e create_document
    /models.py              # ← ORM model para rag.documents table
  /auth
    /audit.py               # ← Audit logging service
  /common
    /s3.py                  # ← S3 client wrapper (boto3)
```

**Frontend Structure**:
```
/frontend/src
  /components
    /documents
      /DocumentUpload.tsx   # ← Componente de upload (drag-and-drop + progress)
      /PendingDocumentsList.tsx  # ← Lista de docs pendentes
  /api
    /documents.ts           # ← API client (axios) para endpoints de documentos
```

**Database Migration**:
```
/backend/alembic/versions
  /001_create_documents_table.py  # ← Nova migration
```

### Lessons Learned from Previous Stories
**N/A** - Esta é a primeira story do projeto (US-RAG-01)

### Security Considerations
1. **Upload Size Limit**: Implementar tanto client-side (UX) quanto server-side (segurança) para evitar DoS via arquivos gigantes
2. **File Type Validation**: Verificar magic bytes do PDF (não apenas extensão) para evitar upload de arquivos maliciosos disfarçados
3. **Rate Limiting**: Considerar rate limiting de uploads (e.g., 10 uploads/hora por admin) para prevenir abuso
4. **Virus Scanning**: (Opcional para MVP) Integrar com ClamAV ou similar para scan de malware antes de processar

### Performance Considerations
1. **Large File Uploads**: Para arquivos próximos de 50MB, considerar upload em chunks (multipart upload do S3) para melhorar resiliência
2. **S3 Transfer Acceleration**: (Opcional) Habilitar S3 Transfer Acceleration para uploads de concessionárias em regiões distantes do datacenter
3. **Processing ETA Calculation**: Estimar tempo de processamento baseado em file size (~1 minuto por 10MB como baseline)

### References

**Tech Specs**:
- Section 1.2: API Specifications (POST /api/v1/documents/upload) - lines 76-157
- Section 1.7: Testing Strategy for Epic 1 - lines 692-806

**Solution Architecture**:
- ADR-006: Deployment Target (AWS S3 storage) - lines 237-265
- Component Breakdown: RAG Module - lines 462-484
- Data Architecture: rag.documents table schema - lines 677-695
- Security Architecture: Encryption and LGPD - lines 899-998

**PRD**:
- FR-05: Ingestão e Processamento de Base de Conhecimento - lines 127-129
- NFR-05: Segurança e Privacidade de Dados (LGPD) - lines 199-204

**Epics**:
- Epic 1: Infraestrutura RAG e Base de Conhecimento - US-RAG-01

---

## Dev Agent Record

### Context Reference
**Context XML Generated**: `/docs/stories/story-context-1.1.xml`
- Generated by: BMAD Story Context Workflow v6
- Generated at: 2025-10-10
- Status: Complete ✅

This XML file contains the complete development context for this story, including:
- All acceptance criteria with technical details
- Task breakdown (13 tasks, 40+ subtasks)
- Relevant documentation snippets from PRD, Architecture, Tech Specs, Epics
- API interface specifications (POST /api/v1/documents/upload, GET /api/v1/documents)
- Architectural constraints (Modular Monolith, Async Processing, S3 Storage)
- Security requirements (RBAC, Encryption, Audit Logging)
- Testing requirements (Unit, Integration, E2E) with test ideas
- Dependencies (boto3, fastapi, react-dropzone, etc.)

### Agent Model Used
**Model**: Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)
**Agent**: Bob (Scrum Master) via BMAD workflow `create-story`
**Date**: 2025-10-10

### Completion Notes List
- Story criada via workflow automatizado (create-story)
- Modo não-interativo (#yolo) ativado - draft gerado sem prompts
- Primeira story do projeto (Epic 1, Story 1)
- Próximo passo: Executar workflow `story-context` para gerar XML de contexto para dev agent

### File List
**Story File**: `/docs/stories/story-1.1.md` (este arquivo)
**Related Files**:
- `/backend/rag/routes.py` (a criar)
- `/backend/rag/services.py` (a criar)
- `/backend/rag/models.py` (a criar)
- `/frontend/src/components/documents/DocumentUpload.tsx` (a criar)
- `/frontend/src/components/documents/PendingDocumentsList.tsx` (a criar)
- `/backend/tests/unit/test_rag_upload.py` (a criar)
- `/backend/tests/integration/test_rag_upload_flow.py` (a criar)
- `/frontend/tests/e2e/document-upload.spec.ts` (a criar)
- `/backend/alembic/versions/001_create_documents_table.py` (a criar)

---

**Last Updated**: 2025-10-10
**Created By**: Bob (BMAD Scrum Master Agent)
**Workflow**: `bmad:bmm:workflows:create-story`
